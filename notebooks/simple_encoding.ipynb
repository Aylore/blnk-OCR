{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = \"ةأىآإ ابتثجحخدذرزسشصضطظعغفقكلمنهوي٠١٢٣٤٥٦٧٨٩0123456789ئءؤ\"\n",
    "# Create a mapping between characters and their indices\n",
    "char2index = {char: idx for idx, char in enumerate(alphabet)}\n",
    "index2char = {idx: char for idx, char in enumerate(alphabet)}\n",
    "char2index[\" \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    # target_tensors =[]\n",
    "   \n",
    "    token_indices = [char2index[char] for char in text]\n",
    "#     target_tensors.append(token_indices)\n",
    "\n",
    "    return token_indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا بكم\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def remove_arabic_diacritics(text):\n",
    "    # Remove Arabic diacritics using Unicode normalization\n",
    "    normalized_text = unicodedata.normalize('NFD', text)\n",
    "    cleaned_text = ''.join([c for c in normalized_text if not unicodedata.combining(c)])\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Sample text with Arabic diacritics\n",
    "text_with_diacritics = \"مَرْحَبًا بِكُم\"\n",
    "\n",
    "# Remove diacritics\n",
    "cleaned_text = remove_arabic_diacritics(text_with_diacritics)\n",
    "\n",
    "print(cleaned_text)  # Output: \"مرحبا بكم\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مَرْحَبًا بِ كُم\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_english_letters(text):\n",
    "    # Define a regular expression pattern to match English letters\n",
    "    pattern = r'[a-zA-Z]'\n",
    "\n",
    "    # Use re.sub() to remove English letters from the text\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Sample text with mixed Arabic and English letters\n",
    "mixed_text = \"مَرْحَبًا بِHello كُم\"\n",
    "\n",
    "# Remove English letters\n",
    "cleaned_text = remove_english_letters(mixed_text)\n",
    "\n",
    "print(cleaned_text)  # Output: \"مَرْحَبًا بِ كُم\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pad_list(lst, desired_length , value):\n",
    "    current_length = len(lst)\n",
    "\n",
    "    if current_length < desired_length:\n",
    "        # Calculate the number of zeros needed\n",
    "        num_zeros = desired_length - current_length\n",
    "\n",
    "        # Add zeros to the end of the list\n",
    "        lst.extend([value] * num_zeros)\n",
    "\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader , random_split\n",
    "import tqdm \n",
    "\n",
    "import torch\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir,  transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.files = os.listdir(data_dir)\n",
    "        self.samples = []\n",
    "        self.alphapet = [\n",
    "                'أ', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي',\n",
    "                '٠', '١', '٢', '٣', '٤', '٥', '٦', '٧', '٨', '٩'\n",
    "            ]\n",
    "        self.max_length = 64\n",
    "    \n",
    "        # self.vectorizer = CountVectorizer(analyzer='char')\n",
    "        for filename in self.files:\n",
    "            if filename.endswith('.jpg'):\n",
    "                base_name = os.path.splitext(filename)[0]\n",
    "                txt_name = base_name + '.txt'\n",
    "                if txt_name in self.files:\n",
    "                    self.samples.append( (filename, txt_name) )# ,CustomDataset.read_text(self.data_dir + txt_name) ))\n",
    "\n",
    "        \n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, txt_name  = self.samples[idx]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "        txt_path = os.path.join(self.data_dir, txt_name)\n",
    "        \n",
    "        image = Image.open(img_path)\n",
    "        with open(txt_path, 'r') as txt_file:\n",
    "            text = txt_file.read()\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # print(txt_raw + \"asdsss\"  )\n",
    "        # print(self.vectorizer.transform([txt_raw]))\n",
    "\n",
    "        # raw_text = CustomDataset.read_text(self.data_dir  + txt_name)\n",
    "        cleaned_text = remove_arabic_diacritics(text)\n",
    "        cleaned_text = remove_english_letters(cleaned_text)\n",
    "        encoded_text = encode(cleaned_text)\n",
    "        # try:\n",
    "        \n",
    "        #     print(encoded_text.shape)\n",
    "        # except Exception as ex:\n",
    "        #     print(f\"asdas {len(encoded_text)}\")\n",
    "\n",
    "\n",
    "        if len(encoded_text) > self.max_length :\n",
    "            encoded_text = encoded_text[:self.max_length]   ## truncate\n",
    "        elif len(encoded_text) < self.max_length:\n",
    "            encoded_text = pad_list(encoded_text ,self.max_length ,char2index[\" \"])                      ## Padd\n",
    "        \n",
    "        encoded_text = torch.tensor(encoded_text)\n",
    "        # print(len(encoded_text) , encoded_text)\n",
    "        return {'image': image, 'text': text, 'embeddings' : encoded_text  }\n",
    "    \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def read_text(txt_path):\n",
    "        with open(txt_path, 'r') as txt_file:\n",
    "                text = txt_file.read()\n",
    "\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your data directory and other relevant parameters\n",
    "data_directory = '../data/OCR_Text_Dataset/OCR_Text/'\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "# Define transformation for the images (you can modify this based on your needs)\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    # transforms.Grayscale() ,\n",
    "        \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create the dataset and data loaders\n",
    "custom_dataset = CustomDataset(data_dir=data_directory, transform=image_transform)\n",
    "\n",
    "train_size = int(0.8 * len(custom_dataset))\n",
    "test_size = len(custom_dataset) - train_size\n",
    "\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = random_split(custom_dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check a batch from dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['image', 'text', 'embeddings']), torch.Size([16, 64]))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "batch.keys() , batch['embeddings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = remove_arabic_diacritics(batch[\"text\"][0])\n",
    "cleaned_text = remove_english_letters(cleaned_text)\n",
    "encoded_text = encode(cleaned_text)\n",
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.batch_norm = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "        self.gru = nn.GRU(256, 256, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Prepare data for the GRU\n",
    "        x = x.squeeze(2)  # Remove the second dimension\n",
    "        x = x.permute(0, 2, 1)  # Permute to (batch_size, seq_len, features)\n",
    "\n",
    "        # GRU layer\n",
    "        x, _ = self.gru(x)\n",
    "\n",
    "        # FC layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Initialize the CRNN model\n",
    "num_classes = len(alphabet)  # Number of output classes (characters)\n",
    "crnn = CRNN(num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[191], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m image_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m     \u001b[39m# transforms.Resize((256, 256)),\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39m# transforms.Grayscale() ,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         \n\u001b[1;32m      6\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m      7\u001b[0m ])\n\u001b[0;32m----> 9\u001b[0m crnn(image_transform(plt\u001b[39m.\u001b[39;49mimread(\u001b[39m\"\u001b[39;49m\u001b[39mblack_text_extracted.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/grad/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[171], line 36\u001b[0m, in \u001b[0;36mCRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m# Prepare data for the GRU\u001b[39;00m\n\u001b[1;32m     35\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m)  \u001b[39m# Remove the second dimension\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m)  \u001b[39m# Permute to (batch_size, seq_len, features)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# GRU layer\u001b[39;00m\n\u001b[1;32m     39\u001b[0m x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),\n",
    "    # transforms.Grayscale() ,\n",
    "        \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "crnn(image_transform(plt.imread(\"black_text_extracted.jpg\")).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (unsigned char) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[176], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m crnn(torchvision\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mread_image(\u001b[39m\"\u001b[39;49m\u001b[39mblack_text_extracted.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/grad/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[171], line 21\u001b[0m, in \u001b[0;36mCRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 21\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     22\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     23\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/grad/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/grad/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/grad/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (unsigned char) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "crnn(torchvision.io.read_image(\"in\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 128]),\n",
       " torch.Size([2, 64]),\n",
       " torch.Size([2, 64]),\n",
       " torch.Size([16, 3, 256, 256]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape , hn.shape , cn.shape , batch[\"image\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /home/aylore/.local/lib/python3.10/site-packages (3.8.0)\n",
      "Requirement already satisfied: torch in /home/aylore/anaconda3/envs/grad/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/aylore/anaconda3/envs/grad/lib/python3.10/site-packages (from h5py) (1.24.3)\n",
      "Requirement already satisfied: filelock in /home/aylore/.local/lib/python3.10/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /home/aylore/anaconda3/envs/grad/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/aylore/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/aylore/.local/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/aylore/.local/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aylore/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/aylore/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(2):\n",
    "    for data in train_dataloader:  # Iterate through your dataset\n",
    "        images = data[\"image\"]\n",
    "        encoded_labels = data[\"embeddings\"]\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs , (_,_) = model(images)\n",
    "        \n",
    "        # Calculate CTC loss\n",
    "        input_lengths = torch.full((batch_size,), outputs.shape[0], dtype=torch.int32)\n",
    "        target_lengths = torch.full((batch_size,), len(encoded_labels[0]), dtype=torch.int32)\n",
    "        loss = loss(outputs, encoded_labels, input_lengths, target_lengths)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def img_to_chars(img_path, output_directory):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply thresholding to create a binary image\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours in the binary image\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Define a function to extract and save individual letters\n",
    "    def extract_letters(contour, img, index):\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        letter = img[y:y+h, x:x+w]\n",
    "\n",
    "        # Resize the letter to a fixed size if needed\n",
    "        letter = cv2.resize(letter, (50, 50))  # Adjust dimensions as needed\n",
    "\n",
    "        # Convert the single-channel grayscale letter to a three-channel RGB image\n",
    "        letter_rgb = cv2.cvtColor(letter, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # Save the letter as an image\n",
    "        letter_filename = os.path.join(output_directory, f'letter_{index}.png')\n",
    "        cv2.imwrite(letter_filename, letter_rgb)\n",
    "\n",
    "    # Process each contour (letter) and save it as a separate image\n",
    "    for i, contour in enumerate(contours):\n",
    "        extract_letters(contour, gray, i)\n",
    "\n",
    "out = img_to_chars(\"inverted_image.jpg\" , \"outputs/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_white_letters(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold the image to create a binary mask of white text on a black background\n",
    "    _, binary_mask = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours in the binary mask\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Initialize a list to store individual letter images\n",
    "    letter_images = []\n",
    "\n",
    "    # Iterate through the contours and extract individual letters\n",
    "    for contour in contours:\n",
    "        # Get the coordinates of the bounding box around the letter\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Extract the individual letter from the image using the bounding box\n",
    "        letter = image[y:y+h, x:x+w]\n",
    "\n",
    "        # Append the letter image to the list\n",
    "        letter_images.append(letter)\n",
    "\n",
    "    return letter_images\n",
    "\n",
    "# Example usage:\n",
    "input_image_path = 'black_text_extracted.jpg'\n",
    "letters = extract_white_letters(input_image_path)\n",
    "\n",
    "# Save each extracted letter as a separate image\n",
    "for i, letter in enumerate(letters):\n",
    "    cv2.imwrite(f'letter_{i}.jpg', letter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image using OpenCV\n",
    "image = cv2.imread('black_text_extracted.jpg')\n",
    "\n",
    "# Convert the image to grayscale if it's in color (optional)\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Invert the colors (reverse black and white)\n",
    "inverted_image = cv2.bitwise_not(image)\n",
    "\n",
    "# # Display the inverted image\n",
    "# cv2.imshow('Inverted Image', inverted_image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# Save the inverted image if needed\n",
    "cv2.imwrite('inverted_image.jpg', inverted_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
